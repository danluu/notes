Notes from reading H&P's Computer Architecture: A Quantitative Approach (5ed)

* Questions
** Chapter 1  
  Why does gate vs. interconnect delay scale the way it does?
  For interconnect, naively, both dimensions shrink, so resistance goes up by S^2
  But length shrinks by S, reducing R by S, and also reducing the C we need to charge?

  For gates, channel shrinks, and so does the capacitance at both ends. 

  When does it make sense to sleep to halt (run very fast so we can enter a low p-state)?

** Chapter 2
   How does way prediction work
** Chapter 3

* Chapter 1: Basic Quant stuff
* Chapter 2: Memory Hierarchy
  Terminology. With a set associative cache, we map a block to a set, and we can then place the block anywhere in the set
  set = [block] % #sets

  Note: A typical thing to do is have a WT l1 and a WB l2

** cache tradeoffs
*** larger block sizes
    lower compulsory miss rate. Also, fewer tags => slightly reduced power
    higher capacity, conflict miss rate
*** bigger cache
    lower capacity miss rate, obv. 
    higher power, possibly longer latency
*** higher associativity
    lower conflict miss rate
    higher power, possibly longer latency
*** multilevel caches
    size, perhaps? Tradeoff is non-obvious
    higher level cache can be bigger and have higher associativity
*** give reads priority over writes
    reduces miss penalty. We do this in CNR
    Note: write buffers create a hazard (read after write), so we should check write buffers on a miss
*** avoid address translation for cache indexing
    page offset has same VA as PA
    removes TLB from critical access path, but limits size of cache
    Usually reasonable for l1?

** cache performance improvements
*** reduce latency
    small (l1) cache, way prediction
    These also decrease power

    Critical path:
    address tag memory using index, compare read tag to address, choose correct item (mux)
    direct mapped caches can overlap tag check with data transmission (way prediction for set associative)

**** way prediction
     Guess the next way, somehow
     Power saving method: only do one tag access. If that misses, try the other tags the next cycle
     typ: 90% for 2-way, 80% for 4-way.

     We could also do the same for the access (not just tag match/selection), but that increases the mispredict penalty
**** Nonblocking
     Nonblocking simply means that we can keep serving hits if there's a miss outstanding
     Note: supporting N misses at some level means you must support at least N misses at each higher level (where does the miss come from?)
     Not quite true for caches shared across processors, of course
    
*** bandwith
    add pipe stages, used banked cache, nonblocking
    Mixed effect on power, but wouldn't you expect these to increase power? Well, it's complicated, because maybe serving things sooner decreases power

    Note: adding pipe stages directly affects branch mispredict penalty
*** reduce miss penalty
    Critical word first, merge write buffers
    Little impact on power
**** critical word first: what it sounds like (must have support on bus)
**** early restart: memory request goes in normal order, but required block gets sent to the processor immediately on read
**** write merging
     Check write buffer to see if we're writing same address again. Merge.
     Obv, important for WT cache. Still some benefit for WB cache
*** reduce miss rate
    compiler / code optimizations (cache blocking, loop interchange to make inner loop tight)
    Obv, compile time improvements help power
*** above two, via parallelism
    hardware and compiler prefetching.
    Usually increase power, due to unused prefetched data

** main memory
*** SRAM: nothing interesting in the text about it
*** DRAM
    recall that addressing is multiplexed (RAS/CAS)
    DIMM usually contains 4-16 chips, output usually 8 bytes wide

    SDRAM: synchronus (DRAM used to be async!)
    burst mode: one address, multiple data

**** Why is GDDR faster than DDR?
     wider interface (32 bit vs 4-16)
     support higher clock rate by soldering chip directly to board; chip talks directly to GPU. Better signal integretiy / less capacitance?

** VMM
   architecture specified by page talbes (x86, IBM VM/370) or TLBs (many RISCs)
   
   Don't do two stage indirection. Instead, keep shadow page tables that map from guest VA to PA
   VMM must trap any attempts to modify page tables, and substitute appropriate modification

   Must also virtualize I/O
   Mechanism varies: disks are usually partititioned, ethernet/wireless can be time sliced, etc.

   Note: OS can be more efficient if it knows it's virtualized (Xen)
    
   Problematic instructions for a VMM:
   SGDT/SLDT/SIDT. Not sure why these are listed as problematic, because they should #GP if we're not at cpl 0
   PUSHF/POPF. POPF ignores IF instead of trapping, PUSHF lets user mode OS see that something strange has happened
   VIP/VIF flags are supposed to work around that problem (user mode manipulates VIF/VIP instead of IF/IP)
      
** TODO: read i7 cache section
   
* Chapter 3: ILP
** dependencies
*** data (true) dependencies
*** name dependency (use same register or memory location, but no data flow)
    antidependency between i and j: j writes, i reads.
    (WAR). This can't happen in most static pipelines, because we read before we write
    output dependence: i and j write the same register
*** control dependency
    Can't (statically) move things before or after branches, because that would add/remove the dependency
** basic compiler optimizations
*** loop unrolling
** advanced branch prediction
   2-bit (saturating counter) predictor schemes only look at branch itself
   Can improve accuracy by looking at other branches 

*** Instead, use two-level (correlating) predictor
   e.g., a (1,2) predictor uses info from the last branch to choose between 2 2-bit predictors
   (m,n) m branches, 2^m predictors

   Simple hardware: m-bit shift register can keep track of state
   branch prediction buffer can be indexed by {branch address (low order bits), m-bit global history}

*** Tournament predictor
    use 2-bit saturating counter to choose bewteen local, global, or hybrid (or something like a loop exit predictor)
** dynamic execution
   basic: scoreboarding (CDC 6600)
   Modification to classic 5-stage pipeline: break up decode into 'issue' and 'read operands'
   issuing is in-order (decode + check for structural hazards)
   read operands is OO (instructions may bypass here; data hazards cause instruction to wait)
   
   more advancing: renaming/tomasulo
   TODO: Skipping reading this section, because it should be pretty familiar

** speculation (note: only skimmed this section, because it's pretty familiar)
   must extend tomasolu's algortithm as follows:
   
   Need a seperate commit stage! So, we can bypass/forward reuslts without saying that something is complete
   Everything has to commit in-order

   ROB holds results between instruction execution and commit (CN calls it complete, not commit)
   Write register file on commit. In classic Tomasolu, RF contains result as soon as possible

   Alternative: instead of holding results in ROB, we have a large rename register file. 
   Architected registers live somewhere in the RF. Change mapping when instruction completes

   How do we decide when to free a register?
   Easy way: when another instruction writes the same architectural register, we can free it
   Hard way: check sources of all outstanding instructions

** static scheduling (skimmed, because VLIW)
   superscalar processors have overhead. VLIW reduces that overhead

** superscalar dynamic scheduling
   Must have logic to handle all possible combinations of dependencies between instructions we're issuing

   Note that this is hard to pipeline away. As new instructions are issued and enter reservation stations, we have to update possible dependencies

** advanced speculation techniques
   how do we deliver a high bandwidth instruction stream (up to 4-8 per clock)?
   BTB: predict branch target based on PC

   Return address prediction: keep a stack of return addresses
   
** hardware vs. software techniques
   Want to disambiguate memory references so we can re-order loads and stores. 
   Very difficult to do statically, in general, but can do dynamically

   dynamic branch prediction highly superior; even in-order processors use dynamic branch prediction

   combining both can have complex effects
   Consider cmov + renaming. Non-move must still copy value to dest, since it was renamed earlier in the pipeline

** threading
   This section covers, SMT, FMT (switch every clock), CMT (switch on stalls), etc. Skimming because it seems familiar

** case study
   TODO: go back and look at this

* Chapter 4: Vector, SIMD, and GPU architectures
  
