Notes from reading H&P's Computer Architecture: A Quantitative Approach (5ed)

* Questions
** Chapter 1  
  Why does gate vs. interconnect delay scale the way it does?
  For interconnect, naively, both dimensions shrink, so resistance goes up by S^2
  But length shrinks by S, reducing R by S, and also reducing the C we need to charge?

  For gates, channel shrinks, and so does the capacitance at both ends. 

  When does it make sense to sleep to halt (run very fast so we can enter a low p-state)?

** Chapter 2
   How does way prediction work

* Chapter 1: Basic Quant stuff
* Chapter 2: Memory Hierarchy
  Terminology. With a set associative cache, we map a block to a set, and we can then place the block anywhere in the set
  set = [block] % #sets

  Note: A typical thing to do is have a WT l1 and a WB l2

** cache tradeoffs
*** larger block sizes
    lower compulsory miss rate. Also, fewer tags => slightly reduced power
    higher capacity, conflict miss rate
*** bigger cache
    lower capacity miss rate, obv. 
    higher power, possibly longer latency
*** higher associativity
    lower conflict miss rate
    higher power, possibly longer latency
*** multilevel caches
    size, perhaps? Tradeoff is non-obvious
    higher level cache can be bigger and have higher associativity
*** give reads priority over writes
    reduces miss penalty. We do this in CNR
    Note: write buffers create a hazard (read after write), so we should check write buffers on a miss
*** avoid address translation for cache indexing
    page offset has same VA as PA
    removes TLB from critical access path, but limits size of cache
    Usually reasonable for l1?

** cache performance improvements
*** reduce latency
    small (l1) cache, way prediction
    These also decrease power

    Critical path:
    address tag memory using index, compare read tag to address, choose correct item (mux)
    direct mapped caches can overlap tag check with data transmission (way prediction for set associative)

**** way prediction
     Guess the next way, somehow
     Power saving method: only do one tag access. If that misses, try the other tags the next cycle
     typ: 90% for 2-way, 80% for 4-way.

     We could also do the same for the access (not just tag match/selection), but that increases the mispredict penalty
**** Nonblocking
     Nonblocking simply means that we can keep serving hits if there's a miss outstanding
     Note: supporting N misses at some level means you must support at least N misses at each higher level (where does the miss come from?)
     Not quite true for caches shared across processors, of course
    
*** bandwith
    add pipe stages, used banked cache, nonblocking
    Mixed effect on power, but wouldn't you expect these to increase power? Well, it's complicated, because maybe serving things sooner decreases power

    Note: adding pipe stages directly affects branch mispredict penalty
*** reduce miss penalty
    Critical word first, merge write buffers
    Little impact on power
**** critical word first: what it sounds like (must have support on bus)
**** early restart: memory request goes in normal order, but required block gets sent to the processor immediately on read
**** write merging
     Check write buffer to see if we're writing same address again. Merge.
     Obv, important for WT cache. Still some benefit for WB cache
*** reduce miss rate
    compiler / code optimizations (cache blocking, loop interchange to make inner loop tight)
    Obv, compile time improvements help power
*** above two, via parallelism
    hardware and compiler prefetching.
    Usually increase power, due to unused prefetched data

** main memory
*** SRAM: nothing interesting in the text about it
*** DRAM
    recall that addressing is multiplexed (RAS/CAS)
    DIMM usually contains 4-16 chips, output usually 8 bytes wide

    SDRAM: synchronus (DRAM used to be async!)
    burst mode: one address, multiple data

**** Why is GDDR faster than DDR?
     wider interface (32 bit vs 4-16)
     support higher clock rate by soldering chip directly to board; chip talks directly to GPU. Better signal integretiy / less capacitance?

** VMM
   architecture specified by page talbes (x86, IBM VM/370) or TLBs (many RISCs)
   
   Don't do two stage indirection. Instead, keep shadow page tables that map from guest VA to PA
   VMM must trap any attempts to modify page tables, and substitute appropriate modification

   Must also virtualize I/O
   Mechanism varies: disks are usually partititioned, ethernet/wireless can be time sliced, etc.

   Note: OS can be more efficient if it knows it's virtualized (Xen)
    
   Problematic instructions for a VMM:
   SGDT/SLDT/SIDT. Not sure why these are listed as problematic, because they should #GP if we're not at cpl 0
   PUSHF/POPF. POPF ignores IF instead of trapping, PUSHF lets user mode OS see that something strange has happened
   VIP/VIF flags are supposed to work around that problem (user mode manipulates VIF/VIP instead of IF/IP)
      
** TODO: read i7 cache section
   
