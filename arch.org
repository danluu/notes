Notes from reading H&P's Computer Architecture: A Quantitative Approach (5ed)

* Questions
** Chapter 1  
  Why does gate vs. interconnect delay scale the way it does?
  For interconnect, naively, both dimensions shrink, so resistance goes up by S^2
  But length shrinks by S, reducing R by S, and also reducing the C we need to charge?

  For gates, channel shrinks, and so does the capacitance at both ends. 

  When does it make sense to sleep to halt (run very fast so we can enter a low p-state)?

** Chapter 2
   How does way prediction work
** Chapter 3
** Chapter 4
   Loop-level parallelism: what does it mean for something to be "loop carried"?
** Chapter 5
   MESIF: what exactly does the 'F' forward?

   See Adve and Gharachorloo [1996] on relaxed consistency models

* Chapter 1: Basic Quant stuff
* Chapter 2: Memory Hierarchy
  Terminology. With a set associative cache, we map a block to a set, and we can then place the block anywhere in the set
  set = [block] % #sets

  Note: A typical thing to do is have a WT l1 and a WB l2

** cache tradeoffs
*** larger block sizes
    lower compulsory miss rate. Also, fewer tags => slightly reduced power
    higher capacity, conflict miss rate
*** bigger cache
    lower capacity miss rate, obv. 
    higher power, possibly longer latency
*** higher associativity
    lower conflict miss rate
    higher power, possibly longer latency
*** multilevel caches
    size, perhaps? Tradeoff is non-obvious
    higher level cache can be bigger and have higher associativity
*** give reads priority over writes
    reduces miss penalty. We do this in CNR
    Note: write buffers create a hazard (read after write), so we should check write buffers on a miss
*** avoid address translation for cache indexing
    page offset has same VA as PA
    removes TLB from critical access path, but limits size of cache
    Usually reasonable for l1?

** cache performance improvements
*** reduce latency
    small (l1) cache, way prediction
    These also decrease power

    Critical path:
    address tag memory using index, compare read tag to address, choose correct item (mux)
    direct mapped caches can overlap tag check with data transmission (way prediction for set associative)

**** way prediction
     Guess the next way, somehow
     Power saving method: only do one tag access. If that misses, try the other tags the next cycle
     typ: 90% for 2-way, 80% for 4-way.

     We could also do the same for the access (not just tag match/selection), but that increases the mispredict penalty
**** Nonblocking
     Nonblocking simply means that we can keep serving hits if there's a miss outstanding
     Note: supporting N misses at some level means you must support at least N misses at each higher level (where does the miss come from?)
     Not quite true for caches shared across processors, of course
    
*** bandwith
    add pipe stages, used banked cache, nonblocking
    Mixed effect on power, but wouldn't you expect these to increase power? Well, it's complicated, because maybe serving things sooner decreases power

    Note: adding pipe stages directly affects branch mispredict penalty
*** reduce miss penalty
    Critical word first, merge write buffers
    Little impact on power
**** critical word first: what it sounds like (must have support on bus)
**** early restart: memory request goes in normal order, but required block gets sent to the processor immediately on read
**** write merging
     Check write buffer to see if we're writing same address again. Merge.
     Obv, important for WT cache. Still some benefit for WB cache
*** reduce miss rate
    compiler / code optimizations (cache blocking, loop interchange to make inner loop tight)
    Obv, compile time improvements help power
*** above two, via parallelism
    hardware and compiler prefetching.
    Usually increase power, due to unused prefetched data

** main memory
*** SRAM: nothing interesting in the text about it
*** DRAM
    recall that addressing is multiplexed (RAS/CAS)
    DIMM usually contains 4-16 chips, output usually 8 bytes wide

    SDRAM: synchronus (DRAM used to be async!)
    burst mode: one address, multiple data

**** Why is GDDR faster than DDR?
     wider interface (32 bit vs 4-16)
     support higher clock rate by soldering chip directly to board; chip talks directly to GPU. Better signal integretiy / less capacitance?

** VMM
   architecture specified by page talbes (x86, IBM VM/370) or TLBs (many RISCs)
   
   Don't do two stage indirection. Instead, keep shadow page tables that map from guest VA to PA
   VMM must trap any attempts to modify page tables, and substitute appropriate modification

   Must also virtualize I/O
   Mechanism varies: disks are usually partititioned, ethernet/wireless can be time sliced, etc.

   Note: OS can be more efficient if it knows it's virtualized (Xen)
    
   Problematic instructions for a VMM:
   SGDT/SLDT/SIDT. Not sure why these are listed as problematic, because they should #GP if we're not at cpl 0
   PUSHF/POPF. POPF ignores IF instead of trapping, PUSHF lets user mode OS see that something strange has happened
   VIP/VIF flags are supposed to work around that problem (user mode manipulates VIF/VIP instead of IF/IP)
      
** TODO: read i7 cache section
   
* Chapter 3: ILP
** dependencies
*** data (true) dependencies
*** name dependency (use same register or memory location, but no data flow)
    antidependency between i and j: j writes, i reads.
    (WAR). This can't happen in most static pipelines, because we read before we write
    output dependence: i and j write the same register
*** control dependency
    Can't (statically) move things before or after branches, because that would add/remove the dependency
** basic compiler optimizations
*** loop unrolling
** advanced branch prediction
   2-bit (saturating counter) predictor schemes only look at branch itself
   Can improve accuracy by looking at other branches 

*** Instead, use two-level (correlating) predictor
   e.g., a (1,2) predictor uses info from the last branch to choose between 2 2-bit predictors
   (m,n) m branches, 2^m predictors

   Simple hardware: m-bit shift register can keep track of state
   branch prediction buffer can be indexed by {branch address (low order bits), m-bit global history}

*** Tournament predictor
    use 2-bit saturating counter to choose bewteen local, global, or hybrid (or something like a loop exit predictor)
** dynamic execution
   basic: scoreboarding (CDC 6600)
   Modification to classic 5-stage pipeline: break up decode into 'issue' and 'read operands'
   issuing is in-order (decode + check for structural hazards)
   read operands is OO (instructions may bypass here; data hazards cause instruction to wait)
   
   more advancing: renaming/tomasulo
   TODO: Skipping reading this section, because it should be pretty familiar

** speculation (note: only skimmed this section, because it's pretty familiar)
   must extend tomasolu's algortithm as follows:
   
   Need a seperate commit stage! So, we can bypass/forward reuslts without saying that something is complete
   Everything has to commit in-order

   ROB holds results between instruction execution and commit (CN calls it complete, not commit)
   Write register file on commit. In classic Tomasolu, RF contains result as soon as possible

   Alternative: instead of holding results in ROB, we have a large rename register file. 
   Architected registers live somewhere in the RF. Change mapping when instruction completes

   How do we decide when to free a register?
   Easy way: when another instruction writes the same architectural register, we can free it
   Hard way: check sources of all outstanding instructions

** static scheduling (skimmed, because VLIW)
   superscalar processors have overhead. VLIW reduces that overhead

** superscalar dynamic scheduling
   Must have logic to handle all possible combinations of dependencies between instructions we're issuing

   Note that this is hard to pipeline away. As new instructions are issued and enter reservation stations, we have to update possible dependencies

** advanced speculation techniques
   how do we deliver a high bandwidth instruction stream (up to 4-8 per clock)?
   BTB: predict branch target based on PC

   Return address prediction: keep a stack of return addresses
   
** hardware vs. software techniques
   Want to disambiguate memory references so we can re-order loads and stores. 
   Very difficult to do statically, in general, but can do dynamically

   dynamic branch prediction highly superior; even in-order processors use dynamic branch prediction

   combining both can have complex effects
   Consider cmov + renaming. Non-move must still copy value to dest, since it was renamed earlier in the pipeline

** threading
   This section covers, SMT, FMT (switch every clock), CMT (switch on stalls), etc. Skimming because it seems familiar

** case study
   TODO: go back and look at this

* Chapter 4: Vector, SIMD, and GPU architectures
** vector processors
   May have a set of vector registers (e.g., 64 x 64-bit registers)
   Highly multi-ported (16 read, 8 write in example), to allow vector ops to different registers at the same time
   May also have a set of scalar registers that can be used as inputs to vector registers (e.g., 32 GPRs and 32 FP regs for 'VMIPS' example)

   Classic example: Y = a * X + Y (SAXPY / DAXPY LINPACK example). 6 VMIPS ops vs. 600 MIPS ops

   Note: even though we have 64-wide regsiters, we may only have, e.g., 4 lanes, so a full op would take 64/4 clocks

   But, if we have, say, 16 lanes and 4 units, we could do 64 / clock, as long as we can keep units occupied

   How do we handle a loop up to n over some wide thing? We have some register that tells us the MVL (maximum vector length).
   Then, we can (effectively) have one loop that handles any number of iterations up to a multiple of the MVL, and one loop for the remainder
   The compiler can probably paramaterize a single loop to handle that.

   How do we handle if statements in loops? 
   Can use vector-mask control (conditional execution for each element in vector). Seems like vectorized cmov-like thing

   Note that GPUs usually don't expose architectural mask registers. Instead, they do something similar internally

   How about vectorizing loads/stores for multi-dimension arrays? Can support stride > 1 in loads/stores.
   Note: this increases the probability of bank contention in main memory

   What about sparse matricies? Gather-scatter ops
   Use some index vector. Gather gets values from those indicies. 
** SIMD
   Unlike vector machines, we don't have a length register that specifies the number of operands
   So, we have many more instructions in the ISA
   Typically don't have strided access, scatter-gather, or conditional execution

   Advantage: much simpler.
   SIMD loads/stores typically can't cross page boundaries, so don't have to worry about faulting in the middle of a vector load/store.
   Don't have to keep state (which makes context switches easier)
   Short fixed-length ops make it easy to add acceleration for particular applications (e.g., some specific permutation op to speed up h.264)
   Lack of strided access makes simple TLB more effective
** GPU
   TODO: go back and read this section. Only skimmed on first reading
** Loop level parallelism
   for(...)
     x[i] = x[i] + s;

   parellel, because dependency is within one iteration, and it isn't "loop carried"

  for(...)
    A[i+1] = A[i] + C[i]
    B[i+1] = B[i] + A[i+1]

  Intra loop dependency can be easily parallelized
  But, Inter-loop dependency (first line, A on prev A) is loop carried

  Note that we can still extract parallelism from some loop carried dependencies
  Consider the following:

  for(...)
    A[i] = A[i] + B[i]
    B[i+1] = C[i] + D[i]

  First line depends on second, but, it's not a cyclic depdendency, so we can still parallelize it
  using the following transform:
  A[0] = ...
  for(...)
    B[i+1] = ...
    A[i+1] = ...
  B[100] = ...
** core i7 vs nvidia GPU:
   TODO: read this section
* Chapter 5: Thread-level parallelism (TLP)
 
  SMP / UMA: usually 8 procs or less
  DSM / NUMA: distributed shared memory. Multiple memory systems => higher bandwidth

  Address space is shared for both of these types of machines (in contrast to clusters and warehouse-scale machines)

** centralized shared memory architectures
   Same idea as caching. Small = fast.
   Chip has local memory via backside bus. Other chips can access by routing through owning chip
*** coherence
**** directory based
     SMP: single centralized directory.
     In a multicore system, this could be the outermost cache

     DSM: More complicated. See section 5.4
**** snooping
     Notes elided because this is familiar
     Note that we can have snooping on top of a directory (snooping between multicores, with a directory for each multicore)

*** coherence protocols
**** MSI: basic
**** MESI
     E: can have non-shared written without generating invalidates
     Optimizes case where a single cache reads a line and then writes it

     Note: Read miss (from another core) to this core's E line must change state to S
**** MESIF (Intel core i series)
     F: Foward. Designates one processes to respond to requests
**** MOESI
     O: line is owned and out of date in memory.
     Can change from M to O without writing back!
     Other blocks can share the line, afterwards, but the Owner must supply the value on a miss
** performance 
   Recall that we have 3Cs for single core caches (capacity, compulsory, conflict).
   For SMP, we have coherence misses (two types)

   True sharing misses
   Examples: first write to a shared block causes an invalidation.
   Reading a modified word in that block causes the block to be transferred

   False sharing misses
   Occurs because we have one state for a whole cache line.
   It's a false miss if different words in a line conflict

   Note: kernel code has a much higher miss rate than user code
   Besides lack of locality and large code size...
   it shares data, so we get coherence misses
   and it initializes pages before allocating them to user code, so we get compulsory misses
** locks, etc.
   TODO: skipping this for now, because it seems familiar
** consistency
   Ordering!
*** Simplest model: sequential consistency
   Memory accesses from one processor are kept in order.
   Memory accesses between different processes are interleaved arbitrarily
*** Simple programming model that's "easy" to implement efficiently:
   Assume programs are syncrhonized.
   This means that every possible write by processor A and read by processor B are seperated by a pair of synchroniziation instructions.
   One on processor A, after the write, and one of processor B, before the read
   (for all A, B, A != B)

   Using read and write locks are one simple example of this.

   Non synchronized accesses are called data races (ordering will depend on relative speed of the processors)

*** Relaxed Consistency
    Allow reads and writes to complete out of order; use synchronization operations to maintain ordering.

    What can we relax?
    1. W->R (total store ordering). 
    2. W->W (partial store ordering).
    3. R->W and R->R (weak ordering or release consitency or something else, depending on sync ops)

    Idea: one argument is that speculation can give you many of the performance advantages for relaxed consistency while maintaining sequential consistency
    Note: the compiler is a key part of that

*** Croscutting issues
**** Compiler optimization
     Defininig a consistency model specifies the range of legal compiler optimizations
     Recall the list of LLVM consistency models
     
     Consider register allocation of shared data. Without clearly defined syncs, we can't interchange a read and a write of 
     *different* shared data items!
    
**** Speculation to hide latency
     We can execute memory references out of order, as long as we commit in order.
     Invalidations will kill speculative references to that address (cache line, most likely)

     We can also replay any cases that will result in a violation of sequential consistency, which should be rare

     Note: compiler optimization of memory refs to shared variables is still an open problem. 
     It's possible that relaxed consistency models will provide a real edge in the future
** Notes after this point on chapter 5 lost due to HD crash
   TODO: re-do notes for those sections
   Two approaches for better performance: either keep sequential consistency and use other techniques to hide latency, or use relaxed consistency models

* Chapter 6: Warehouse-scale computers (WSC)
  Notes on first few sections lost due to HD crash
  TODO: re-do notes for those sections

 
